{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist.test.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = 784\n",
    "x = tf.placeholder(tf.float32, [None, D])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([D, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution from Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9179\n"
     ]
    }
   ],
   "source": [
    "# tf.argmax(y,1) - what we predicted\n",
    "# tf.argmax(y_,1) - correct label\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "See: https://github.com/pilipolio/schibsted-study/blob/master/notebooks/201612_mnist.ipynb\n",
    "\n",
    "### Task\n",
    "* Re-user softmax classification from https://github.com/pilipolio/schibsted-study/blob/master/notebooks/201611_multiclass_classification.ipynb\n",
    "* Define train_X and train_Y from mnist_data.train.images/labels\n",
    "* Calculate the accuracy (ratio of correctly classified labels)\n",
    "* Think about a way to visualise the fitted weights\n",
    "* Improve the accuracy of the naive model by adding non-linear layers (see https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "WIDTH, HEIGHT = (28, 28)\n",
    "\n",
    "# unique_labels = np.unique(mnist.train.labels)\n",
    "unique_labels = 10\n",
    "# print('Train has {} dimensions with {} unique classe: {}'.format(mnist.train.images.shape, len(unique_labels), unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples, D = mnist.train.images.shape\n",
    "C = unique_labels #.shape[0]\n",
    "\n",
    "n_samples, D, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUkAAAFKCAYAAACQHq0GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAACM9JREFUeJzt3T2IlWcexuHnyAiDCiIy4xibEFAQxMCMEPCjlKCVjU0i\nKsFKmEZkAhYWhkACKRQsbQRj4UdsDUlhpUWYKaYRLMTGQsRCUjg44tlid9ldWG//r/HEc5zrar0Z\nHoz55SHy8Pb6/X4D4P9b9aEPADDMRBIgEEmAQCQBApEECEQSIBBJgEAkAYKxyqjX621srX3ZWnvU\nWlsa5IEA/ibjrbVPW2u/9vv9Z28alSLZ/hnIn9/DoQCGzdettatv+sVqJB+11tqVK1fa9u3b38OZ\nAD6s+/fvtyNHjrT2r769STWSS621tn379jY9Pf3XTgYwXOL/QvQXNwCBSAIEIgkQiCRAIJIAgUgC\nBCIJEIgkQCCSAIFIAgQiCRCIJEAgkgCBSAIEIgkQiCRAIJIAgUgCBCIJEIgkQCCSAIFIAgQiCRCI\nJEAgkgCBSAIEIgkQiCRAIJIAgUgCBCIJEIgkQCCSAIFIAgQiCRCIJEAgkgCBSAIEIgkQiCRAIJIA\nwdiHPgD/0e/3y9uffvqpvL169Wp5u2fPnvJ2amqqvJ2cnCxvP//88/J227Zt5e2GDRvK21HT5c9O\na60tLy+Xt3/88Ud52+XPz6hwkwQIRBIgEEmAQCQBApEECEQSIBBJgEAkAQKRBAhEEiDwLHEFWFxc\nHMh2GPR6vfJ2dna2vP3+++/L2zVr1pS3g9Ll96G11lavXl3e7t69u+txPipukgCBSAIEIgkQiCRA\nIJIAgUgCBCIJEIgkQCCSAIFIAgSeJY6oU6dOlbdHjhwZyBm6fKHvwYMH5e3atWvL27Nnz5a3Fy5c\nKG+7fFlxbm6uvB0fHy9vB6nrM8aVzE0SIBBJgEAkAQKRBAhEEiAQSYBAJAECkQQIRBIgEEmAwLPE\nIdLlqdiqVfX/vk1NTb3Lcd6rzZs3D+Tn3rx5s7xdt25deXvu3LnydteuXeXtwYMHy1uGg5skQCCS\nAIFIAgQiCRCIJEAgkgCBSAIEIgkQiCRAIJIAgUgCBN5uj6hR+yRol8/PLi0tlbdHjx59l+O81fr1\n68vbLVu2DOQMDAc3SYBAJAECkQQIRBIgEEmAQCQBApEECEQSIBBJgEAkAQLPEhk6169fL29v3bo1\nkDPMzs6Wtzt37hzIGRgObpIAgUgCBCIJEIgkQCCSAIFIAgQiCRCIJEAgkgCBSAIEniXyP7p81fDl\ny5fl7eLiYnn7448/lrddbNq0qbw9efJkeTtqX66kGzdJgEAkAQKRBAhEEiAQSYBAJAECkQQIRBIg\nEEmAQCQBAs8SR1SX54PLy8vl7fz8fHm7f//+8vbFixflbReTk5Pl7cLCQnk7MTHxLsfhI+QmCRCI\nJEAgkgCBSAIEIgkQiCRAIJIAgUgCBCIJEIgkQOBZ4gB1eTrYWmvPnz8vb7t8ze/Jkyfl7Z07d8rb\nYXDgwIHydmpqqrz1BUT+zU0SIBBJgEAkAQKRBAhEEiAQSYBAJAECkQQIRBIgEEmAwLPEIfLs2bPy\n9ubNm+Xtq1ev3uU4I+Hy5cvl7caNG8vbM2fOlLcbNmwobxk9bpIAgUgCBCIJEIgkQCCSAIFIAgQi\nCRCIJEAgkgCBSAIEniUOUNcv7n322Wfl7fz8fHl77dq18nbHjh3l7aFDh8rb1atXl7ddvjL5+++/\nl7ezs7Pl7dLSUnl79uzZ8nZiYqK8ZTi4SQIEIgkQiCRAIJIAgUgCBCIJEIgkQCCSAIFIAgQiCRD0\nKk/Aer3edGttfn5+vk1PTw/+VLxVl6d7w6DrE82qLr8PXbbffPNNeXvv3r3y9u7du+Vtl6870t3C\nwkKbmZlprbWZfr+/8KadmyRAIJIAgUgCBCIJEIgkQCCSAIFIAgQiCRCIJEAgkgCBryWOqEE98xs1\ng/p9OHPmTHm7b9++8va7774rb8+fP1/eMjhukgCBSAIEIgkQiCRAIJIAgUgCBCIJEIgkQCCSAIFI\nAgSeJbJidHnCuHXr1vL2xIkT5e3FixfL27m5ufL2k08+KW/pxk0SIBBJgEAkAQKRBAhEEiAQSYBA\nJAECkQQIRBIgEEmAwLNE+Iv27t1b3v7www/l7cOHD8tbzxIHx00SIBBJgEAkAQKRBAhEEiAQSYBA\nJAECkQQIRBIgEEmAwLNEhk6/3y9vFxYWytuZmZl3Oc5bXb58eSA/l+HgJgkQiCRAIJIAgUgCBCIJ\nEIgkQCCSAIFIAgQiCRCIJEDgWeKI6vJ07/Xr1+Xt48ePy9vffvutvL19+3Z5e/fu3fL26dOn5e2f\nf/5Z3p4+fbq8/eWXX8rbXq9X3o6N+ddzGLhJAgQiCRCIJEAgkgCBSAIEIgkQiCRAIJIAgUgCBCIJ\nEHj3tAIsLi6Wt7t27RrgST6s8fHx8nbVqsHcH6anp8vbL774YiBnoBs3SYBAJAECkQQIRBIgEEmA\nQCQBApEECEQSIBBJgEAkAQKRBAi83V4BlpaWPvQRBqbLp3W7fM5106ZN5e23335b3h49erS87XJe\nBsdNEiAQSYBAJAECkQQIRBIgEEmAQCQBApEECEQSIBBJgMCzxBWgy2diL126VN7euHGjvL19+3Z5\n28W+ffvK28OHD5e3x44dK2/XrVtX3jJ63CQBApEECEQSIBBJgEAkAQKRBAhEEiAQSYBAJAECkQQI\nPEscUV2+pDc2Vv/HfPz48YFsP2a+avhxc5MECEQSIBBJgEAkAQKRBAhEEiAQSYBAJAECkQQIRBIg\n8CxxBfBsDt6dmyRAIJIAgUgCBCIJEIgkQCCSAIFIAgQiCRCIJEAgkgCBSAIEIgkQiCRAIJIAgUgC\nBCIJEIgkQCCSAIFIAgQiCRCIJEAgkgCBSAIEIgkQiCRAIJIAgUgCBCIJEIgkQCCSAIFIAgRjxd14\na63dv39/gEcB+Pv8V8/G067X7/ff+sN6vd5XrbWf//qxAIbO1/1+/+qbfrEayY2ttS9ba49aa0vv\n7WgAH854a+3T1tqv/X7/2ZtGpUgCrFT+4gYgEEmAQCQBApEECEQSIBBJgEAkAYJ/AHLDGL3i3/YK\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126534c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_image(pixels):\n",
    "    plt.imshow(pixels.reshape((WIDTH, HEIGHT)), cmap='Greys', interpolation='None');\n",
    "    plt.xticks([]); plt.yticks([]);\n",
    "\n",
    "show_image(mnist.train.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = .5\n",
    "n_epochs = 1000\n",
    "display_step = 100\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0, cost=0.3913, %correct 93.0\n",
      "Epoch:  100, cost=0.1394, %correct 98.0\n",
      "Epoch:  200, cost=0.4720, %correct 91.0\n",
      "Epoch:  300, cost=0.1726, %correct 97.0\n",
      "Epoch:  400, cost=0.1996, %correct 95.0\n",
      "Epoch:  500, cost=0.3291, %correct 93.0\n",
      "Epoch:  600, cost=0.2684, %correct 93.0\n",
      "Epoch:  700, cost=0.3076, %correct 91.0\n",
      "Epoch:  800, cost=0.4067, %correct 92.0\n",
      "Epoch:  900, cost=0.4118, %correct 89.0\n"
     ]
    }
   ],
   "source": [
    "# storing weights for each epochs for later visualisation\n",
    "fitted_ws = np.zeros((n_epochs, D * unique_labels))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    # batch_xs = mnist.train.images\n",
    "    # batch_ys = mnist.train.labels\n",
    "    _, fitted_w, loss_value, predicted_probs = sess.run(\n",
    "        fetches=[optimizer, W, cross_entropy, y],\n",
    "        feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    fitted_ws[epoch, :] = fitted_w.ravel()\n",
    "    predicted_probs = sess.run(y, feed_dict={x: batch_xs})\n",
    "    predicted_classes = predicted_probs.argmax(axis=1)\n",
    "    n_correct_samples = np.sum(predicted_classes == batch_ys.argmax(axis=1))\n",
    "    percent_correct = round(n_correct_samples / len(predicted_classes) * 100)\n",
    "    \n",
    "    if (epoch) % display_step == 0:\n",
    "        print(\"Epoch: {:4d}, cost={:.4f}, %correct {}\".format(epoch, loss_value, percent_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUkAAAFKCAYAAACQHq0GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngIKSBCgoSYCCkgQo/A+jm/d6UPtLYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1289eee10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w_images = fitted_ws[-1].reshape((D, 10))\n",
    "show_image(w_images[:,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9233\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv-net solution using TF Slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lenet(images):\n",
    "   net = slim.layers.conv2d(images, 20, [5,5], scope='conv1')\n",
    "   net = slim.layers.max_pool2d(net, [2,2], scope='pool1')\n",
    "   net = slim.layers.conv2d(net, 50, [5,5], scope='conv2')\n",
    "   net = slim.layers.max_pool2d(net, [2,2], scope='pool2')\n",
    "   net = slim.layers.flatten(net, scope='flatten3')\n",
    "   net = slim.layers.fully_connected(net, 500, scope='fully_connected4')\n",
    "   net = slim.layers.fully_connected(net, 10, activation_fn=None, scope='fully_connected5')\n",
    "   return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.reshape((-1, HEIGHT, WIDTH, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images = mnist.train.images.reshape((-1, HEIGHT, WIDTH, 1))\n",
    "net = lenet(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Rank mismatch: Labels rank (received %s) should equal logits rank (received %s) - 1.', 2, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-476ad2a035e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/olegshevelev/Soft/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[0;34m(logits, labels, name)\u001b[0m\n\u001b[1;32m    558\u001b[0m       raise ValueError(\"Rank mismatch: Labels rank (received %s) should equal \"\n\u001b[1;32m    559\u001b[0m                        \u001b[0;34m\"logits rank (received %s) - 1.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m                        labels_static_shape.ndims, logits.get_shape().ndims)\n\u001b[0m\u001b[1;32m    561\u001b[0m     \u001b[0;31m# Check if no reshapes are required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('Rank mismatch: Labels rank (received %s) should equal logits rank (received %s) - 1.', 2, 2)"
     ]
    }
   ],
   "source": [
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(net, mnist.train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
